# -*- coding: utf-8 -*-
"""Food-Image-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107XlLKSdMwVlAZLlFbaAzajXOJmWNrl6
"""

from google.colab import drive
drive.mount('/content/drive')

!ls drive/MyDrive/IIITH-Courses/CV/Image-Classification/datasets
DATA_PATH = "drive/MyDrive/IIITH-Courses/CV/Image-Classification/datasets"

"""# Imports"""

import matplotlib.pyplot as plt
import os
import torch
import pandas as pd
from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
import tqdm
import cv2
from PIL import Image

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
from torch.utils.data.sampler import SubsetRandomSampler
from torch import nn, optim
from torchvision import transforms, utils, datasets

"""# Dataset Creation and Loader

To build a custom dataset and dataloader : [Reference](https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html)


ToTensor converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]. This is required to convert all images to tensors in our dataloader. [Ref](https://towardsdatascience.com/pytorch-vision-binary-image-classification-d9a227705cf9)
"""

food_id2name = {}
food_name2id = {}

food_i = -1
with open(os.path.join(DATA_PATH, "dataset_info.txt"), 'r') as f:

    line = f.readline().strip()

    while(line):
        if food_i < 0 :
            food_i+=1
            line = f.readline().strip()
            continue

        num, name = line.split(" ")
        num = int(num[:-1])
        food_id2name[num] = name
        food_name2id[name] = num

        food_i+=1
        line = f.readline().strip()

len(food_name2id)

class FoodDataset(Dataset):

    def __init__(self, csv_file, root_dir, dataset_type, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.food_df = pd.read_csv(csv_file)
        #self.food_df = self.food_df.head(100)
        self.root_dir = root_dir
        self.transform = transform
        self.dataset_type = dataset_type

    def __len__(self):
        return len(self.food_df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = os.path.join(self.root_dir, self.food_df.iloc[idx, 0])
        #image = io.imread(img_name)
        image = Image.open(img_name)
        #image = cv2.cvtColor(cv2.imread(img_name), cv2.COLOR_BGR2RGB)
        #image = cv2.resize(image, (256,256))

        if self.dataset_type == "train":
            food_class = torch.tensor(food_name2id[self.food_df.iloc[idx, 1]])
            sample = {'image': image, 'food_class': food_class}
        else:
            sample = {'image': image}

        if self.transform:
            sample["image"] = self.transform(sample["image"])

        return sample

"""# Data Transformations

A suite of transformations used at training time is typically referred to as data augmentation and is a common practice for modern model development.
It performs the transformations on fly in each iteration. So it will not increase the actual scale of your data on your disk
"""

image_transforms1 = {
    "train": transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.RandomAffine(0, shear=10, scale = (0.8, 1.2)),
        transforms.ColorJitter(brightness=0.2, contrast = 0.2, saturation =0.2),
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ]),
    "test": transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])
}

image_transforms2 = {
    "train": transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ]),
    "test": transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])
}

food_train_dataset = FoodDataset(csv_file=os.path.join(DATA_PATH , "train.csv"), 
                                 root_dir=os.path.join(DATA_PATH , "train_images"), 
                                 dataset_type="train",
                                 transform = image_transforms1["train"]
                                 )

food_test_dataset = FoodDataset(csv_file=os.path.join(DATA_PATH , "test.csv"), 
                                root_dir=os.path.join(DATA_PATH , "test_images"), 
                                dataset_type="test",
                                 transform = image_transforms1["test"]
                                )

"""# Analyzing the Train and Test Data"""

print("Total train samples =", len(food_train_dataset))
print("Total test samples =", len(food_test_dataset))

fig = plt.figure(figsize = (10,10))

for i in range(0, 5):
    sample = food_train_dataset[i]
    ax = plt.subplot(1, 5, i + 1)
    plt.tight_layout()
    ax.set_title('Train sample #{}'.format(sample['food_class']))
    ax.axis('off')
    ax.imshow(sample["image"].permute(1, 2, 0))   #EACH IMAGE IS OF THE SHAPE = (C x H x W)

food_train_dataset.food_df.value_counts("ClassName", normalize=True).plot(x="ClassName", y="count", kind="bar", fontsize=12, figsize=(40,10))

#create the train validation and test dataloaders

BATCH_SIZE = 128
VAL_SPLIT = 0.2
shuffle_dataset = True
random_seed= 42

# Creating data indices for training and validation splits:
dataset_size = len(food_train_dataset)
indices = list(range(len(food_train_dataset)))
split = int(np.floor(VAL_SPLIT * dataset_size))

if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, val_indices = indices[split:], indices[:split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

train_dataloader = DataLoader(food_train_dataset, batch_size=BATCH_SIZE, num_workers=4, sampler=train_sampler)
val_dataloader = DataLoader(food_train_dataset, batch_size=BATCH_SIZE, num_workers=4, sampler=val_sampler)
test_dataloader = DataLoader(food_test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)

print("Total train batches =", len(train_dataloader))
print("Total validation batches =", len(val_dataloader))
print("Total test batches =", len(test_dataloader))

"""# Build Image Classifier Model

Pretrained Model Options : 
* [Resnet](https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5)

Model from scratch Option (Ideas):
* 2 conv layers : [Link](https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/)
* More complex architechture : [Link](https://thevatsalsaglani.medium.com/training-and-deploying-a-multi-label-image-classifier-using-pytorch-flask-reactjs-and-firebase-c39c96f9c427)

* [CNN Ref](https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8)

* [Digit Classifier](https://towardsdatascience.com/convolutional-neural-network-for-image-classification-with-implementation-on-python-using-pytorch-7b88342c9ca9)

* [CNN2 Ref](https://medium.com/analytics-vidhya/image-classification-with-convolutional-neural-networks-ac14a978f0fa)

* [Binary Class CNN](https://github.com/Haylemicheal/Cats-vs-Dogs-Classifier/blob/master/Cats%20vs%20Dogs%20classifier.ipynb)

"""

# defining the model architecture
class FoodClassifier(nn.Module):   
    def __init__(self):
        super(FoodClassifier, self).__init__()

        '''
        Architechture option 1
        '''
        # self.cnn_layers = nn.Sequential(
        #     # Defining a 2D convolution layer
        #     nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1),
        #     nn.BatchNorm2d(4),
        #     nn.ReLU(inplace=True),
        #     nn.MaxPool2d(kernel_size=2, stride=2),
        #     # Defining another 2D convolution layer
        #     nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),
        #     nn.BatchNorm2d(4),
        #     nn.ReLU(inplace=True),
        #     nn.MaxPool2d(kernel_size=2, stride=2),
        # )

        # self.linear_layers = nn.Sequential(
        #     nn.Linear(4 * 64 * 64, 62)
        #)

        '''
        Architechture option 2
        '''
        self.ConvLayer1 = nn.Sequential(nn.Conv2d(3, 32, 3, stride=1, padding=1), 
                                        nn.MaxPool2d(2), 
                                        #nn.BatchNorm2d(32),
                                        nn.ReLU(), 
                                        nn.Dropout(0.3)
                                        )
        self.ConvLayer2 = nn.Sequential(nn.Conv2d(32, 64, 3, stride=1, padding=1),
                                        nn.MaxPool2d(2), 
                                        #nn.BatchNorm2d(64),
                                        nn.ReLU(),
                                        nn.Dropout(0.3)
                                        )
        self.ConvLayer3 = nn.Sequential(nn.Conv2d(64, 128, 3, stride=1, padding=1), 
                                        nn.MaxPool2d(2), 
                                        #nn.BatchNorm2d(128),
                                        nn.ReLU(), 
                                        nn.Dropout(0.3)
                                        )
        self.ConvLayer4 = nn.Sequential(nn.Conv2d(128, 256, 3, stride=1, padding=1),
                                        nn.MaxPool2d(2), 
                                        #nn.BatchNorm2d(256),
                                        nn.ReLU(), 
                                        nn.Dropout(0.3)
                                        )
        
        self.ConvLayer5 = nn.Sequential(nn.Conv2d(256, 512, 3, stride=1, padding=1),
                                        nn.MaxPool2d(2), 
                                        #nn.BatchNorm2d(512),
                                        nn.ReLU(), 
                                        nn.Dropout(0.3)
                                        )
        
        self.LinearLayers = nn.Sequential(nn.Linear(512 * 8 * 8, 1024),
                                          nn.Linear(1024, 512),
                                          nn.Linear(512, 256),
                                          nn.Linear(256, 62),
                                          #nn.LogSoftmax(dim=1)
                                        )


    # Defining the forward pass    
    def forward(self, x):
        '''
        Forward pass option 1 
        '''
        # x = self.cnn_layers(x)
        # x = x.view(x.size(0), -1)
        # x = self.linear_layers(x)
        # return x

        '''
        Forward pass option 2
        '''
        #print("ori x ", x.shape)
        x = self.ConvLayer1(x)
        #print("layer 1", x.shape)
        x = self.ConvLayer2(x)
        #print("layer 2", x.shape)
        x = self.ConvLayer3(x)
        #print("layer 3", x.shape)
        x = self.ConvLayer4(x)
        #print("layer 4", x.shape)
        x = self.ConvLayer5(x)
        #print("layer 5", x.shape)
        x = x.view(x.size(0), -1)
        x = self.LinearLayers(x)
        return x

# checking if GPU is available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# defining the model, optimizer, loss func
model = FoodClassifier().to(device)
#optimizer = optim.Adam(model.parameters(), lr=0.01)
# optimizer = optim.SGD(model.parameters(), lr = 0.0001, momentum = 0.9, weight_decay = 5e-5)
optimizer = optim.SGD(model.parameters(), lr = 0.007, momentum = 0.95, weight_decay = 1e-5)

#loss_fn = nn.NLLLoss().to(device) #negative log likelihood loss function is better as it can capture the softmax layer
loss_fn = nn.CrossEntropyLoss().to(device)

'''
nn.CrossEntropyLoss expects integer labels. What it does internally is that it doesn't end up one-hot encoding 
the class label at all, but uses the label to index into the output probability vector to calculate the loss 
should you decide to use this class as the final label. This small but important detail makes computing the loss 
easier and is the equivalent operation to performing one-hot encoding, measuring the output loss per output neuron 
as every value in the output layer would be zero with the exception of the neuron indexed at the target class. 
Therefore, there's no need to one-hot encode your data if you have the labels already provided.
'''


print(model)

"""# Training the Model on train mages"""

EPOCHS = 50

train_losses = []
val_losses = []
val_loss_min = np.Inf

for epoch in tqdm.tqdm(range(EPOCHS)):

    train_loss = 0
    val_loss = 0

    #trainign on the train dataset
    model.train()
    for batch in (train_dataloader):
        
        images = batch["image"]
        labels = batch["food_class"]

        if torch.cuda.is_available():
            images = images.to(device)
            labels = labels.to(device)

        # Training pass
        optimizer.zero_grad()
        output = model(images)
        #_, predicted = torch.max(torch.exp(output.data), 1)

        #print("out ", output.shape, "lab ", labels.shape, "pred", predicted.shape)
        #print(output)
        #print(predicted)
        #print(labels)

        loss = loss_fn(output, labels)

        #print(loss.item(),"loss")
        #print(images.size(0), "sizes")
        #print("loss term = ", loss.item()*images.size(0))

        #print(torch.exp(output[0]))
        #print(output.data[0])
        #print(output[0])

        #This is where the model learns by backpropagating
        loss.backward()

        #And optimizes its weights here
        optimizer.step()
        train_loss += loss.item()*images.size(0)

    #evaluating on the validation dataset
    model.eval()
    with torch.no_grad():
        for batch in (val_dataloader):
            
            images = batch["image"]
            labels = batch["food_class"]

            if torch.cuda.is_available():
                images = images.to(device)
                labels = labels.to(device)

            output = model(images)
            loss = loss_fn(output, labels)
            val_loss += loss.item()*images.size(0)

    train_loss = train_loss/len(train_dataloader.sampler)
    val_loss = val_loss/len(val_dataloader.sampler)
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(epoch, train_loss, val_loss))
    
    if val_loss <= val_loss_min:
        print("Validation Loss decreased {:0.6f} -> {:0.6f}".format(val_loss_min,val_loss))
        print()
        val_loss_min = val_loss
        torch.save(model.state_dict(), os.path.join(DATA_PATH , 'best_model.pth'))
    print()
    #print("Epoch {} - Training loss: {}".format(i+1, running_loss/len(train_dataloader)))

plt.plot(train_losses, label='Training loss')
plt.plot(val_losses, label='Validation loss')
plt.legend(frameon=False)
plt.show()
plt.savefig("loss_plot.png")

model.load_state_dict(torch.load(os.path.join(DATA_PATH,'best_model_so_far.pth')))
model.eval()
correct = 0
total = 0
pred_list = []
correct_list = []

with torch.no_grad():
    for batch in (val_dataloader):
        images = batch['image'].to(device)
        labels = batch['food_class'].to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        pr = predicted.detach().cpu().numpy()
        for i in pr:
          pred_list.append(i)
        tg = labels.detach().cpu().numpy()
        for i in tg:
          correct_list.append(i)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 1920 test images: %f %%' % (
    100 * correct / total))

inputtt = torch.randn((2, 3))
print(inputtt)

mmm = nn.LogSoftmax(dim = 0)
outputtt = mmm(inputtt)
print(outputtt)

np.log((np.exp(-0.2152))/(np.exp(-0.2152) + np.exp(-1.0940)))



"""# Model Tracking

0. **Model 0** Acc 26.394% with BatchedNorm, LR = 0.0001, mom = 0.9, wd = 5e-5, Dropout at only last layer

1. **Model 1** Acc 32.296137% without BatchedNorm, LR = 0.007, mom = 0.95, wd = 1e-5, Dropout at only last layer 0.2

2. **Model 2** without BatchedNorm, LR = 0.007, mom = 0.95, wd = 1e-5, Dropout on all layers 0.3, additional CNN layer, additional Linear Layer

3. **Model 3** without BatchedNorm, LR = 0.007, mom = 0.95, wd = 1e-5, Dropout on all layers 0.5, additional layer 
"""

